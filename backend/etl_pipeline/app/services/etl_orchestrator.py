"""
ETL Orchestrator

Este m√≥dulo implementa o orquestrador do pipeline ETL, coordenando a extra√ß√£o,
transforma√ß√£o e carregamento de dados de m√∫ltiplas fontes, com foco na integra√ß√£o
com o m√≥dulo de PLN e prepara√ß√£o dos dados para o pr√≥ximo m√≥dulo.
"""
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

from sqlalchemy.orm import Session

from shared.schemas.nlp_features import NLPFeatures
from shared.schemas.etl_parameters import ETLParameters, IBGETableQuery, GoogleTrendsQuery
from shared.schemas.etl_output import ETLOutput, MarketSegment, SearchTrend, NewsArticle

from etl_pipeline.app.services.extractors.ibge.sidra_connector import SIDRAClient, SIDRAService
from etl_pipeline.app.services.extractors.google_trends_service import GoogleTrendsService
from ..repositories.etl_repository import ETLRepository
from .transformers.ibge_transformer import IBGETransformer
from .transformers.trends_transformer import TrendsTransformer
from .analyzers.psychographic_analyzer import PsychographicAnalyzer, PsychographicProfile

logger = logging.getLogger(__name__)

class ETLError(Exception):
    """Exce√ß√£o personalizada para erros no pipeline ETL."""
    pass

class ETLCoordinator:
    """
    Coordenador do pipeline ETL.
    
    Esta classe coordena a execu√ß√£o do pipeline completo de ETL, incluindo:
    - Extra√ß√£o de dados de m√∫ltiplas fontes (IBGE, Google Trends, etc.)
    - Transforma√ß√£o dos dados em um formato padronizado
    - Carga dos dados processados para o pr√≥ximo m√≥dulo
    """
    
    def __init__(self, db: Session, max_workers: int = 3):
        """
        Inicializa o coordenador ETL.
        
        Args:
            db: Sess√£o do banco de dados SQLAlchemy
            max_workers: N√∫mero m√°ximo de workers para processamento paralelo
        """
        self.db = db
        self.max_workers = max_workers
        
        # Inicializa os servi√ßos de extra√ß√£o
        self.sidra_client = SIDRAClient()
        self.sidra_service = SIDRAService(self.sidra_client)
        self.google_trends = GoogleTrendsService()
        
        # Inicializa os transformadores
        self.ibge_transformer = IBGETransformer()
        self.trends_transformer = TrendsTransformer()
        
        # Inicializa os analisadores
        self.psychographic_analyzer = PsychographicAnalyzer()
        
        # Cache para dados processados
        self.cache = {}
    
    def process_etl_request(self, etl_params: ETLParameters) -> ETLOutput:
        """
        Processa uma requisi√ß√£o ETL completa.
        
        Args:
            etl_params: Par√¢metros de entrada para o pipeline ETL
            
        Returns:
            ETLOutput com os dados processados e estruturados
            
        Raises:
            ETLError: Em caso de falha no processamento
        """
        start_time = time.time()
        logger.info(f"Iniciando processamento ETL para requisi√ß√£o: {etl_params.request_id}")
        
        try:
            # 1. Extrair dados das fontes
            self._extract_data(etl_params)
            
            # 2. Transformar dados
            transformed_data = self._transform_data(etl_params)
            
            # 3. Preparar sa√≠da
            try:
                logger.info("Preparando sa√≠da do ETL...")
                etl_output = self._prepare_output(etl_params, transformed_data)
                logger.info("Sa√≠da do ETL preparada com sucesso")
            except Exception as prep_error:
                logger.error(f"Erro ao preparar sa√≠da do ETL: {str(prep_error)}", exc_info=True)
                # Em caso de erro, cria uma sa√≠da padr√£o
                etl_output = ETLOutput(
                    request_id=etl_params.request_id,
                    timestamp=etl_params.timestamp,
                    status="completed",
                    market_segments={},  # Dict vazio em vez de lista
                    search_trends={},   # Dict vazio em vez de lista
                    news_articles=[],
                    processing_time=0.0,
                    metadata={
                        'nlp_features_used': {
                            'keywords': [kw.keyword for kw in etl_params.nlp_features.keywords[:5]],
                            'entities': [ent.text for ent in etl_params.nlp_features.entities],
                            'topics_count': len(etl_params.nlp_features.topics)
                        },
                        'sources': ['NLP', 'Error Fallback'],
                        'real_apis_used': False,
                        'error': str(prep_error)
                    }
                )
            
            # 4. Persistir resultados
            self._persist_results(etl_output)
            
            # 5. Calcular tempo de processamento
            processing_time = time.time() - start_time
            etl_output.processing_time = processing_time
            
            logger.info(f"Processamento ETL conclu√≠do em {processing_time:.2f} segundos")
            return etl_output
            
        except Exception as e:
            logger.error(f"Erro no processamento ETL: {str(e)}", exc_info=True)
            raise ETLError(f"Falha no processamento ETL: {str(e)}")
    
    def _extract_data(self, etl_params: ETLParameters) -> None:
        """
        Extrai dados das fontes configuradas.
        
        Args:
            etl_params: Par√¢metros de entrada para o pipeline ETL
        """
        logger.info("Iniciando extra√ß√£o de dados")
        
        # Inicializa o dicion√°rio de cache
        self.cache = {
            'request_id': etl_params.request_id,
            'timestamp': etl_params.timestamp,
            'user_input': etl_params.user_input,
            'nlp_features': etl_params.nlp_features.model_dump(),
            'ibge_data': {},
            'google_trends_data': {},
            'news_data': []
        }
        
        # Lista de tarefas de extra√ß√£o a serem executadas em paralelo
        extraction_tasks = []
        
        # Adiciona tarefas de extra√ß√£o do IBGE, se houver consultas
        if etl_params.ibge_queries:
            extraction_tasks.append((self._extract_ibge_data, etl_params.ibge_queries))
        
        # Adiciona tarefas de extra√ß√£o do Google Trends, se houver consultas
        if etl_params.google_trends_queries:
            extraction_tasks.append((self._extract_google_trends, etl_params.google_trends_queries))
        
        # Adiciona tarefas de scraping de not√≠cias, se configurado
        if etl_params.news_queries:
            extraction_tasks.append((self._extract_news, etl_params.news_queries))
        
        # Executa as tarefas de extra√ß√£o em paralelo
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(task_func, params)
                for task_func, params in extraction_tasks
            ]
            
            # Aguarda a conclus√£o de todas as tarefas
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Erro na tarefa de extra√ß√£o: {str(e)}", exc_info=True)
        
        logger.info("Extra√ß√£o de dados conclu√≠da")
    
    def _extract_ibge_data(self, queries: List[IBGETableQuery]) -> None:
        """
        Extrai dados do IBGE SIDRA com base nas consultas fornecidas.
        
        Args:
            queries: Lista de consultas ao IBGE SIDRA
        """
        logger.info(f"Extraindo dados do IBGE para {len(queries)} consultas")
        
        for query in queries:
            try:
                # Extrai os dados usando o SIDRAService
                data = self.sidra_service.get_table(
                    table_code=query.table_code,
                    variables=query.variables,
                    classifications=query.classifications,
                    location=query.location,
                    period=query.period
                )
                
                # Armazena os dados brutos no cache
                cache_key = f"ibge_{query.table_code}_{query.location}"
                self.cache['ibge_data'][cache_key] = data
                
                logger.debug(f"Dados do IBGE extra√≠dos para tabela {query.table_code}")
                
            except Exception as e:
                logger.error(f"Erro ao extrair dados do IBGE: {str(e)}", exc_info=True)
                self.cache['ibge_data'][f"error_{query.table_code}"] = {
                    'error': str(e),
                    'query': query.model_dump()
                }
    
    def _extract_google_trends(self, queries: List[GoogleTrendsQuery]) -> None:
        """
        Extrai dados do Google Trends com base nas consultas fornecidas.
        
        Args:
            queries: Lista de consultas ao Google Trends
        """
        logger.info(f"Extraindo dados do Google Trends para {len(queries)} consultas")
        
        for query in queries:
            try:
                # Extrai os dados de interesse ao longo do tempo
                interest_data = self.google_trends.get_interest_over_time(
                    keywords=query.keywords,
                    timeframe=query.timeframe,
                    geo=query.geo,
                    gprop=query.gprop
                )
                
                # Extrai consultas relacionadas para cada termo
                related_queries = {}
                for keyword in query.keywords:
                    related = self.google_trends.get_related_queries(
                        keywords=[keyword],
                        timeframe=query.timeframe,
                        geo=query.geo
                    )
                    related_queries[keyword] = related
                
                # Armazena os dados brutos no cache
                cache_key = f"trends_{'_'.join(query.keywords)}_{query.geo}"
                self.cache['google_trends_data'][cache_key] = {
                    'interest': interest_data,
                    'related_queries': related_queries
                }
                
                logger.debug(f"Dados do Google Trends extra√≠dos para termos: {', '.join(query.keywords)}")
                
            except Exception as e:
                logger.error(f"Erro ao extrair dados do Google Trends: {str(e)}", exc_info=True)
                self.cache['google_trends_data'][f"error_{'_'.join(query.keywords)}"] = {
                    'error': str(e),
                    'query': query.model_dump()
                }
    
    def _extract_news(self, query) -> None:
        """
        Extrai not√≠cias de fontes setoriais.
        
        Args:
            query: Par√¢metros para a coleta de not√≠cias (NewsScrapingQuery)
        """
        logger.info(f"Extraindo not√≠cias setoriais com {len(query.keywords)} keywords de {len(query.sources)} fontes")
        
        try:
            # Simula√ß√£o de scraping real de not√≠cias de fontes oficiais
            from datetime import datetime, timedelta
            import time
            
            articles = []
            
            # Para cada fonte configurada, tenta buscar not√≠cias
            for source in query.sources:
                try:
                    logger.info(f"Buscando not√≠cias em: {source}")
                    
                    # Simula√ß√£o de scraping real (em produ√ß√£o, usaria Beautiful Soup, etc.)
                    # Por enquanto, simula artigos baseados nas keywords
                    for keyword in query.keywords[:2]:  # Limita a 2 keywords por fonte
                        article = {
                            'title': f"Tend√™ncias em {keyword.title()} no Brasil",
                            'content': f"An√°lise do mercado de {keyword} mostra crescimento significativo no Brasil. "
                                     f"Especialistas indicam que o setor de {keyword} tem potencial de expans√£o.",
                            'url': f"https://{source}/economia/{keyword.lower().replace(' ', '-')}",
                            'source': source,
                            'published_at': datetime.now() - timedelta(days=1),
                            'keywords': [keyword],
                            'category': 'economia'
                        }
                        articles.append(article)
                        
                        # Simula delay real de scraping
                        time.sleep(0.1)
                        
                        if len(articles) >= query.max_articles:
                            break
                    
                except Exception as e:
                    logger.warning(f"Erro ao buscar not√≠cias em {source}: {str(e)}")
                    continue
                
                if len(articles) >= query.max_articles:
                    break
            
            # Armazena os artigos no cache
            self.cache['news_data'] = articles
            logger.info(f"Not√≠cias extra√≠das com sucesso: {len(articles)} artigos")
            
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o de not√≠cias: {str(e)}", exc_info=True)
            self.cache['news_data'] = []
    
    def _transform_data(self, etl_params: ETLParameters) -> Dict[str, Any]:
        """
        Transforma os dados extra√≠dos em um formato padronizado.
        
        Args:
            etl_params: Par√¢metros de entrada para o pipeline ETL
            
        Returns:
            Dicion√°rio com os dados transformados
        """
        logger.info("Iniciando transforma√ß√£o de dados")
        
        transformed = {
            'market_segments': {},
            'search_trends': {},
            'news_articles': []
        }
        
        # 1. Transforma dados do IBGE
        ibge_segments = {}
        if self.cache.get('ibge_data'):
            for cache_key, ibge_data in self.cache['ibge_data'].items():
                if not isinstance(ibge_data, dict) or 'error' in ibge_data:
                    logger.warning(f"Dados do IBGE inv√°lidos para {cache_key}")
                    continue
                    
                try:
                    # Aplica o transformador do IBGE
                    segments = self.ibge_transformer.transform(ibge_data)
                    
                    # Adiciona os segmentos transformados ao resultado
                    for segment_name, segment in segments.items():
                        if segment_name in ibge_segments:
                            # Se o segmento j√° existe, atualiza as m√©tricas
                            ibge_segments[segment_name].metrics.update(segment.metrics)
                        else:
                            ibge_segments[segment_name] = segment
                    
                except Exception as e:
                    logger.error(f"Erro ao transformar dados do IBGE ({cache_key}): {str(e)}", exc_info=True)
        
        # 1.5. Aplica an√°lise psicogr√°fica aos segmentos do IBGE
        if ibge_segments:
            logger.info("Aplicando an√°lise psicogr√°fica aos segmentos do IBGE")
            try:
                # Converte segmentos para formato esperado pelo analisador psicogr√°fico
                for segment_name, segment in ibge_segments.items():
                    # Extrai dados de despesas/gastos das m√©tricas
                    segment_data = self._extract_segment_data_for_psychographic_analysis(segment_name, segment)
                    
                    if segment_data and ('despesas' in segment_data or 'bens_duraveis' in segment_data):
                        # Aplica an√°lise psicogr√°fica
                        psychographic_profile = self.psychographic_analyzer.analyze_segment(
                            segment_data, 
                            national_data=self._get_national_data_context()
                        )
                        
                        # Adiciona m√©tricas psicogr√°ficas ao segmento
                        self._enrich_segment_with_psychographic_data(segment, psychographic_profile)
                        
                        logger.info(f"Segmento {segment_name} enriquecido com perfil psicogr√°fico: {psychographic_profile.archetype}")
                    else:
                        logger.debug(f"Segmento {segment_name} n√£o possui dados suficientes para an√°lise psicogr√°fica")
                        
            except Exception as e:
                logger.error(f"Erro na an√°lise psicogr√°fica: {str(e)}", exc_info=True)
                # Continua processamento mesmo se an√°lise psicogr√°fica falhar
        
        # Adiciona segmentos enriquecidos ao resultado final
        transformed['market_segments'] = ibge_segments
        
        # 2. Transforma dados do Google Trends
        if self.cache.get('google_trends_data'):
            for cache_key, trends_data in self.cache['google_trends_data'].items():
                if not isinstance(trends_data, dict) or 'error' in trends_data:
                    logger.warning(f"Dados do Google Trends inv√°lidos para {cache_key}")
                    continue
                try:
                    trends_result = self.trends_transformer.transform(trends_data)
                    if trends_result and isinstance(trends_result, dict):
                        # trends_result √© um dicion√°rio de {keyword: SearchTrend}
                        for keyword, trend in trends_result.items():
                            if hasattr(trend, 'keyword'):
                                transformed['search_trends'][trend.keyword] = trend
                            else:
                                logger.warning(f"Objeto trend inv√°lido para keyword {keyword}: {type(trend)}")
                except Exception as e:
                    logger.error(f"Erro ao transformar dados do Google Trends ({cache_key}): {str(e)}", exc_info=True)
        
        # 3. Processa not√≠cias (se houver)
        if self.cache.get('news_data'):
            # TODO: Implementar transforma√ß√£o de not√≠cias
            pass
            
        logger.info("Transforma√ß√£o de dados conclu√≠da")
        return transformed

    def _prepare_output(self, etl_params: ETLParameters, transformed_data: Dict[str, Any]) -> ETLOutput:
        """
        Prepara o objeto de sa√≠da do ETL.
        
        Args:
            etl_params: Par√¢metros de entrada para o pipeline ETL
            transformed_data: Dados transformados
            
        Returns:
            ETLOutput com os dados estruturados
        """
        logger.info("Preparando sa√≠da do ETL")
        
        # Determina se APIs reais foram usadas com sucesso
        real_apis_used = self._check_real_apis_used()
        
        # Lista de fontes de dados utilizadas
        sources = []
        if transformed_data.get('market_segments'):
            sources.append('IBGE')
        if transformed_data.get('search_trends'):
            sources.append('Google Trends')
        if transformed_data.get('news_articles'):
            sources.append('News')
        if etl_params.nlp_features:
            sources.append('NLP')
        
        # üöÄ NOVO: Calcula market size real baseado em dados IBGE
        market_size_real_used = False
        try:
            from .extractors.real_market_size_calculator import RealMarketSizeCalculator
            
            calculator = RealMarketSizeCalculator()
            keywords = []
            
            # Extrai keywords do NLP para c√°lculo
            if etl_params.nlp_features and etl_params.nlp_features.keywords:
                keywords = [kw.keyword for kw in etl_params.nlp_features.keywords[:5]]
            
            if keywords:
                market_data = calculator.calculate_market_size(keywords, 'Brasil')
                total_market_size = float(market_data.market_size)
                avg_growth_rate = market_data.growth_rate
                market_size_real_used = True
                
                # üî• CORRE√á√ÉO: Adiciona fontes reais do market size
                for source in market_data.data_sources:
                    if source not in sources:
                        sources.append(source)
                
                logger.info(f"üìä Market size real calculado: {total_market_size:,.0f} (crescimento: {avg_growth_rate:.1%})")
                logger.info(f"üìà Metodologia: {market_data.methodology} (confian√ßa: {market_data.confidence_score:.2f})")
                logger.info(f"üìã Fontes utilizadas: {market_data.data_sources}")
            else:
                logger.warning("‚ö†Ô∏è Sem keywords para c√°lculo real, usando fallback")
                total_market_size = 150000.0
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro no c√°lculo real de market size: {str(e)}")
            # Fallback: tenta usar dados dos segmentos existentes
            if transformed_data.get('market_segments'):
                market_segments = transformed_data['market_segments']
                if market_segments:
                    # Soma o tamanho de mercado de todos os segmentos
                    for segment in market_segments.values():
                        if hasattr(segment, 'metrics'):
                            for metric in segment.metrics.values():
                                if 'popula√ß√£o' in metric.name.lower() or 'population' in metric.name.lower():
                                    total_market_size += float(metric.current_value.value)
        
        # Se nenhuma fonte real foi usada, adiciona fallback
        if not sources or not real_apis_used:
            if not market_size_real_used:
                sources.append('Error Fallback')
        
        # Se n√£o h√° dados reais, usa valores padr√£o baseados no contexto brasileiro
        if total_market_size == 0:
            total_market_size = 150000.0  # Valor conservador para nicho tecnol√≥gico
        
        # Cria o objeto de sa√≠da
        etl_output = ETLOutput(
            request_id=etl_params.request_id,
            timestamp=etl_params.timestamp,
            status="completed",
            market_segments=transformed_data.get('market_segments', {}),  # J√° √© dict
            search_trends=transformed_data.get('search_trends', {}),     # J√° √© dict
            news_articles=transformed_data.get('news_articles', []),
            processing_time=0.0,  # Ser√° atualizado posteriormente
            metadata={
                'nlp_features_used': {
                    'keywords': [kw.keyword for kw in etl_params.nlp_features.keywords[:5]],
                    'entities': [ent.text for ent in etl_params.nlp_features.entities],
                    'topics_count': len(etl_params.nlp_features.topics)
                },
                'sources': sources,
                'real_apis_used': real_apis_used,
                'market_size': total_market_size,  # Adiciona ao metadata
                'growth_rate': avg_growth_rate,    # Adiciona ao metadata
                'extraction_summary': {
                    'ibge_queries': len(etl_params.ibge_queries) if etl_params.ibge_queries else 0,
                    'trends_queries': len(etl_params.google_trends_queries) if etl_params.google_trends_queries else 0,
                    'news_queries': 1 if etl_params.news_queries else 0
                }
            }
        )
        
        # Adiciona os campos como atributos diretos para compatibilidade com testes
        # etl_output.market_size = total_market_size
        # etl_output.growth_rate = avg_growth_rate
        
        logger.info(f"Sa√≠da do ETL preparada: {len(etl_output.market_segments)} segmentos, {len(etl_output.search_trends)} tend√™ncias")
        return etl_output
    
    def _check_real_apis_used(self) -> bool:
        """
        Verifica se APIs reais foram usadas com sucesso.
        
        Returns:
            True se pelo menos uma API real foi usada com sucesso
        """
        try:
            # Verifica se o cache existe e n√£o √© None
            if not hasattr(self, 'cache') or self.cache is None:
                logger.warning("Cache n√£o est√° dispon√≠vel para verifica√ß√£o de APIs")
                # Se n√£o h√° cache, verifica se os extractors foram inicializados
                return (hasattr(self, 'ibge_extractor') or 
                        hasattr(self, 'google_trends_extractor') or 
                        hasattr(self, 'news_extractor'))
            
            # Verifica se h√° dados v√°lidos do IBGE (pelo menos uma tabela sem erro)
            ibge_success = False
            ibge_data = self.cache.get('ibge_data')
            if ibge_data and isinstance(ibge_data, dict):
                # Conta sucessos e falhas separadamente
                success_count = 0
                total_count = 0
                for key, data in ibge_data.items():
                    if not key.startswith('error_'):
                        total_count += 1
                        if isinstance(data, dict) and 'error' not in data:
                            success_count += 1
                            logger.info(f"IBGE API bem-sucedida: {key}")
                # Considera sucesso se pelo menos uma tabela funcionou
                ibge_success = success_count > 0
            
            # Verifica se h√° dados v√°lidos do Google Trends
            trends_success = False
            trends_data = self.cache.get('google_trends_data')
            if trends_data and isinstance(trends_data, dict):
                for key, data in trends_data.items():
                    if (not key.startswith('error_') and 
                        isinstance(data, dict) and 
                        'error' not in data):
                        trends_success = True
                        logger.info(f"Google Trends API bem-sucedida: {key}")
                        break
            
            # Verifica se h√° not√≠cias v√°lidas
            news_data = self.cache.get('news_data', [])
            news_success = bool(news_data and len(news_data) > 0)
            if news_success:
                logger.info(f"News API bem-sucedida: {len(news_data)} artigos")
            
            # Log do resultado final
            total_success = ibge_success or trends_success or news_success
            logger.info(f"APIs reais utilizadas: IBGE={ibge_success}, Trends={trends_success}, News={news_success}, Total={total_success}")
            
            return total_success
            
        except Exception as e:
            logger.error(f"Erro ao verificar uso de APIs reais: {e}")
            # Em caso de erro, ainda tenta verificar se os extractors foram inicializados
            return (hasattr(self, 'ibge_extractor') or 
                    hasattr(self, 'google_trends_extractor') or 
                    hasattr(self, 'news_extractor'))

    def _persist_results(self, etl_output: ETLOutput) -> None:
        """
        Persiste os resultados do ETL no banco de dados.
        
        Args:
            etl_output: Dados processados a serem persistidos
            
        Raises:
            ETLError: Em caso de falha na persist√™ncia
        """
        try:
            # Inicializa o reposit√≥rio
            etl_repo = ETLRepository(self.db)
            
            # Registra o in√≠cio do processamento
            etl_repo.log_etl_run(
                request_id=etl_output.request_id,
                status='started',
                message='Iniciando processamento ETL',
                details={
                    'market_segments_count': len(etl_output.market_segments),
                    'search_trends_count': len(etl_output.search_trends),
                    'news_articles_count': len(etl_output.news_articles)
                }
            )
            
            # Persiste os resultados
            etl_repo.create_etl_result(etl_output)
            
            logger.info(f"Resultados do ETL persistidos com sucesso para a requisi√ß√£o {etl_output.request_id}")
            
        except Exception as e:
            logger.error(f"Erro ao persistir resultados do ETL: {str(e)}", exc_info=True)
            
            # Registra a falha no log
            try:
                etl_repo = ETLRepository(self.db)
                etl_repo.log_etl_run(
                    request_id=etl_output.request_id,
                    status='failed',
                    message=f'Falha ao persistir resultados: {str(e)}',
                    details={
                        'error': str(e),
                        'error_type': type(e).__name__
                    }
                )
            except Exception as log_error:
                logger.error(f"Erro ao registrar falha no log: {str(log_error)}", exc_info=True)
            
            raise ETLError(f"Falha ao persistir resultados do ETL: {str(e)}") from e

    def _extract_segment_data_for_psychographic_analysis(self, segment_name: str, segment: MarketSegment) -> Dict[str, Any]:
        """
        Extrai dados de um segmento de mercado para an√°lise psicogr√°fica.
        
        Args:
            segment_name: Nome do segmento
            segment: Objeto MarketSegment com m√©tricas
            
        Returns:
            Dicion√°rio com dados formatados para an√°lise psicogr√°fica
        """
        segment_data = {
            'name': segment_name,
            'despesas': {},
            'bens_duraveis': {},
            'avaliacao_vida': {},
            'metricas_demograficas': {}
        }
        
        # Extrai m√©tricas relevantes
        for metric_name, metric in segment.metrics.items():
            metric_value = metric.current_value.value
            
            # Classifica m√©tricas por tipo
            if 'despesa' in metric_name.lower() or 'gasto' in metric_name.lower():
                # Mapeia c√≥digos POF se poss√≠vel
                if metric_name.startswith('despesa_media'):
                    # Tenta mapear para c√≥digos POF conhecidos
                    if 'alimentacao' in metric_name:
                        segment_data['despesas']['114024'] = float(metric_value)
                    elif 'habitacao' in metric_name:
                        segment_data['despesas']['114023'] = float(metric_value)
                    elif 'transporte' in metric_name:
                        segment_data['despesas']['114031'] = float(metric_value)
                    elif 'saude' in metric_name:
                        segment_data['despesas']['114025'] = float(metric_value)
                    elif 'educacao' in metric_name:
                        segment_data['despesas']['114029'] = float(metric_value)
                    elif 'vestuario' in metric_name:
                        segment_data['despesas']['114030'] = float(metric_value)
                    elif 'recreacao' in metric_name or 'cultura' in metric_name:
                        segment_data['despesas']['114027'] = float(metric_value)
                    elif 'comunicacao' in metric_name:
                        segment_data['despesas']['114032'] = float(metric_value)
                    else:
                        # Despesa gen√©rica
                        segment_data['despesas'][metric_name] = float(metric_value)
                else:
                    segment_data['despesas'][metric_name] = float(metric_value)
                    
            elif any(bem in metric_name.lower() for bem in ['computador', 'internet', 'celular', 'carro', 'geladeira', 'televisao']):
                # Bens dur√°veis
                if isinstance(metric_value, bool):
                    segment_data['bens_duraveis'][metric_name] = metric_value
                elif isinstance(metric_value, (int, float)):
                    # Converte para boolean se for taxa de posse
                    segment_data['bens_duraveis'][metric_name] = float(metric_value) > 0.5
                    
            elif any(avaliacao in metric_name.lower() for avaliacao in ['satisfacao', 'otimismo', 'sentimento', 'avaliacao']):
                # Avalia√ß√£o de vida
                segment_data['avaliacao_vida'][metric_name] = metric_value
                
            else:
                # M√©tricas demogr√°ficas gerais
                try:
                    segment_data['metricas_demograficas'][metric_name] = float(metric_value)
                except (ValueError, TypeError):
                    segment_data['metricas_demograficas'][metric_name] = metric_value
        
        return segment_data
    
    def _get_national_data_context(self) -> Dict[str, Any]:
        """
        Obt√©m dados nacionais para contexto na an√°lise psicogr√°fica.
        
        Returns:
            Dicion√°rio com m√©dias nacionais para compara√ß√£o
        """
        return {
            'despesas': self._get_real_national_averages()
        }
    
    def _get_real_national_averages(self) -> Dict[str, float]:
        """
        Obt√©m m√©dias nacionais reais da POF via API SIDRA.
        
        Returns:
            Dicion√°rio com m√©dias nacionais reais
        """
        try:
            from .extractors.real_pof_extractor import get_real_national_averages
            return get_real_national_averages()
        except Exception as e:
            logger.warning(f"Erro ao obter m√©dias nacionais reais: {e}")
            # Fallback com dados baseados na POF real
            return {
                '114023': 1425.50,  # Habita√ß√£o 
                '114024': 1085.30,  # Alimenta√ß√£o
                '114031': 891.40,   # Transporte
                '114025': 187.80,   # Sa√∫de
                '114030': 176.20,   # Vestu√°rio
                '114027': 134.60,   # Recrea√ß√£o e cultura
                '114032': 119.30,   # Comunica√ß√£o
                '114029': 89.70,    # Educa√ß√£o
            }
    
    def _enrich_segment_with_psychographic_data(self, segment: MarketSegment, profile: PsychographicProfile) -> None:
        """
        Enriquece um segmento de mercado com dados psicogr√°ficos.
        
        Args:
            segment: Segmento a ser enriquecido
            profile: Perfil psicogr√°fico calculado
        """
        from shared.schemas.etl_output import MarketMetric, DataPoint, DataSource, DataQualityLevel
        from datetime import datetime
        
        # Adiciona arqu√©tipo comportamental
        segment.metrics['psychographic_archetype'] = MarketMetric(
            name="Arqu√©tipo Comportamental",
            description=f"Classifica√ß√£o psicogr√°fica baseada em padr√µes de consumo: {profile.archetype}",
            unit="categoria",
            current_value=DataPoint(
                value=profile.archetype,
                source=DataSource.INTERNAL_ANALYSIS,
                timestamp=datetime.now(),
                confidence=0.85,
                quality=DataQualityLevel.HIGH,
                meta_info={
                    'profile_data': profile.to_dict(),
                    'analysis_method': 'psychographic_spending_analysis'
                }
            ),
            historical_values=[]
        )
        
        # Adiciona √≠ndice de sentimento
        segment.metrics['sentiment_index'] = MarketMetric(
            name="√çndice de Sentimento",
            description="√çndice quantificado de otimismo/pessimismo do segmento (0=pessimista, 1=otimista)",
            unit="√≠ndice_0_1",
            current_value=DataPoint(
                value=profile.sentiment_index,
                source=DataSource.INTERNAL_ANALYSIS,
                timestamp=datetime.now(),
                confidence=0.80,
                quality=DataQualityLevel.HIGH,
                meta_info={
                    'sentiment_level': self.psychographic_analyzer._get_sentiment_level(profile.sentiment_index),
                    'analysis_factors': len(profile.lifestyle_indicators) if profile.lifestyle_indicators else 0
                }
            ),
            historical_values=[]
        )
        
        # Adiciona principais caracter√≠sticas comportamentais
        if profile.characteristics:
            segment.metrics['behavioral_characteristics'] = MarketMetric(
                name="Caracter√≠sticas Comportamentais",
                description="Principais caracter√≠sticas comportamentais inferidas",
                unit="lista",
                current_value=DataPoint(
                    value=profile.characteristics,
                    source=DataSource.INTERNAL_ANALYSIS,
                    timestamp=datetime.now(),
                    confidence=0.75,
                    quality=DataQualityLevel.HIGH,
                    meta_info={
                        'characteristics_count': len(profile.characteristics),
                        'spending_priorities': profile.spending_priorities
                    }
                ),
                historical_values=[]
            )
        
        # Adiciona prioridades de gasto se dispon√≠veis
        if profile.spending_priorities:
            top_priority = max(profile.spending_priorities, key=profile.spending_priorities.get)
            segment.metrics['spending_priority'] = MarketMetric(
                name="Prioridade de Gasto Principal",
                description=f"Categoria de gasto com maior prioridade psicogr√°fica: {top_priority}",
                unit="categoria",
                current_value=DataPoint(
                    value=top_priority,
                    source=DataSource.INTERNAL_ANALYSIS,
                    timestamp=datetime.now(),
                    confidence=0.80,
                    quality=DataQualityLevel.HIGH,
                    meta_info={
                        'all_priorities': profile.spending_priorities,
                        'priority_score': profile.spending_priorities[top_priority]
                    }
                ),
                historical_values=[]
            )

