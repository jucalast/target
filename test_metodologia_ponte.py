#!/usr/bin/env python3
"""
Teste da Metodologia da "Ponte de TraduÃ§Ã£o Inteligente"

Este script verifica se todo o processo descrito na metodologia estÃ¡ sendo
implementado corretamente, validando cada etapa da "ponte" entre NLP e ETL.
"""
import sys
import os

# Adiciona o backend ao path
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from shared.schemas.nlp_features import NLPFeatures, KeywordFeature, EntityFeature, TopicFeature, EmbeddingFeature
from shared.schemas.etl_parameters import ETLParameters, IBGETableQuery, GoogleTrendsQuery, NewsScrapingQuery
from etl_pipeline.app.services.extractors.ibge.sidra_mapper import SIDRAMapper
from nlp_processor.app.services.nlp_service import extract_features
import json
from datetime import datetime

def test_metodologia_ponte():
    """
    Testa todo o processo da "Ponte de TraduÃ§Ã£o Inteligente" descrito na metodologia.
    """
    print("ğŸ”¬ TESTE DA METODOLOGIA: PONTE DE TRADUÃ‡ÃƒO INTELIGENTE")
    print("=" * 70)
    
    # ========================================================================
    # ETAPA 1: PROCESSAMENTO NLP - GERAÃ‡ÃƒO DO "DOSSIÃŠ DE BUSCA"
    # ========================================================================
    print("\nğŸ“ ETAPA 1: GERAÃ‡ÃƒO DO DOSSIÃŠ DE BUSCA (NLP)")
    print("-" * 50)
    
    # Input do usuÃ¡rio (dados reais)
    niche = "Tecnologia educacional para crianÃ§as"
    description = """
    Plataforma online de ensino de programaÃ§Ã£o e robÃ³tica educativa para crianÃ§as 
    de 6 a 12 anos, focada em escolas pÃºblicas e particulares do Brasil, 
    especialmente nas regiÃµes Sul e Sudeste, com Ãªnfase em famÃ­lias de classe 
    mÃ©dia interessadas em educaÃ§Ã£o STEAM.
    """
    
    print(f"ğŸ¯ Nicho: {niche}")
    print(f"ğŸ“„ DescriÃ§Ã£o: {description.strip()[:100]}...")
    
    # Executa o processamento NLP REAL
    nlp_result = extract_features(niche, description)
    
    # Converte para NLPFeatures estruturado
    nlp_features = NLPFeatures(
        original_text=f"{niche}. {description}",
        normalized_text=nlp_result.get('normalized_text', ''),
        keywords=[
            KeywordFeature(
                keyword=kw['keyword'],
                score=kw['score'],
                method='tfidf'
            ) for kw in nlp_result.get('keywords', [])
        ],
        entities=[
            EntityFeature(
                text=ent['text'],
                label=ent['label'],
                start_char=ent.get('start', 0),
                end_char=ent.get('end', 0)
            ) for ent in nlp_result.get('entities', [])
        ],
        topics=[
            TopicFeature(
                topic_id=i,
                keywords=[
                    {"word": kw, "score": 0.5} if isinstance(kw, str) 
                    else kw for kw in topic.get('keywords', [])
                ],
                score=topic.get('score', 0.0)
            ) for i, topic in enumerate(nlp_result.get('topics', []))
        ],
        embeddings={
            model: EmbeddingFeature(
                model=model,
                vector=vector if isinstance(vector, list) else [],
                dim=len(vector) if isinstance(vector, list) else 0
            )
            for model, vector in nlp_result.get('embeddings', {}).items()
            if isinstance(vector, list)  # Filtra apenas os vetores reais
        },
        processing_time=1.0
    )
    
    print(f"âœ… Palavras-chave extraÃ­das: {len(nlp_features.keywords)}")
    print(f"   Top 5: {[k.keyword for k in nlp_features.keywords[:5]]}")
    print(f"âœ… Entidades extraÃ­das: {len(nlp_features.entities)}")
    print(f"   Entidades: {[f'{e.text} ({e.label})' for e in nlp_features.entities[:3]]}")
    print(f"âœ… TÃ³picos extraÃ­dos: {len(nlp_features.topics)}")
    print(f"âœ… Embeddings gerados: {list(nlp_features.embeddings.keys())}")
    
    # Identifica entidades geogrÃ¡ficas especificamente
    geographic_entities = [e.text for e in nlp_features.entities if e.label in ['LOC', 'GPE']]
    print(f"ğŸŒ Entidades geogrÃ¡ficas: {geographic_entities}")
    
    # ========================================================================
    # ETAPA 2: TRADUÃ‡ÃƒO PARA PARÃ‚METROS SIDRA (MAPPER)
    # ========================================================================
    print("\nğŸ”„ ETAPA 2: TRADUÃ‡ÃƒO PARA PARÃ‚METROS SIDRA")
    print("-" * 50)
    
    # Extrai termos para mapeamento
    keywords_for_mapping = [k.keyword for k in nlp_features.keywords[:10]]
    
    print(f"ğŸ“‹ Input para mapeamento: {keywords_for_mapping}")
    
    # Executa o mapeamento REAL
    mapper = SIDRAMapper()
    sidra_params = mapper.map_terms_to_sidra_parameters(keywords_for_mapping)
    
    print(f"ğŸ¯ Tabela SIDRA selecionada: {sidra_params.get('tabela', 'N/A')}")
    print(f"ğŸ“Š ParÃ¢metros mapeados:")
    
    matched_terms = sidra_params.get('matched_terms', {})
    for param_code, param_info in matched_terms.items():
        print(f"   - ParÃ¢metro {param_code}: {param_info.get('name', 'N/A')}")
        print(f"     Termos: {param_info.get('matched_terms', [])}")
    
    # ========================================================================
    # ETAPA 3: CONSTRUÃ‡ÃƒO DE PARÃ‚METROS ETL (ORQUESTRAÃ‡ÃƒO)
    # ========================================================================
    print("\nğŸ—ï¸ ETAPA 3: CONSTRUÃ‡ÃƒO DE PARÃ‚METROS ETL")
    print("-" * 50)
    
    # 3.1 Filtro geogrÃ¡fico baseado em entidades
    location = "Brasil"
    if geographic_entities:
        if any(geo.lower() in ['sul', 'sudeste', 'nordeste', 'norte', 'centro-oeste'] 
               for geo in geographic_entities):
            location = geographic_entities[0]
        elif any(geo.lower() == 'brasil' for geo in geographic_entities):
            location = "Brasil"
    
    print(f"ğŸŒ Filtro geogrÃ¡fico aplicado: {location}")
    
    # 3.2 CriaÃ§Ã£o de consultas IBGE baseadas na traduÃ§Ã£o
    ibge_queries = [
        IBGETableQuery(
            table_code=sidra_params.get('tabela', '6401'),
            variables=sidra_params.get('variaveis', []),
            classifications=sidra_params.get('classificacoes', {}),
            location=location,
            period='2022'
        )
    ]
    
    print(f"ğŸ“Š Consultas IBGE criadas: {len(ibge_queries)}")
    print(f"   - Tabela: {ibge_queries[0].table_code}")
    print(f"   - LocalizaÃ§Ã£o: {ibge_queries[0].location}")
    
    # 3.3 CriaÃ§Ã£o de consultas Google Trends baseadas em keywords
    top_keywords = [k.keyword for k in nlp_features.keywords[:5]]
    google_trends_queries = [
        GoogleTrendsQuery(
            keywords=top_keywords[:3],  # MÃ¡ximo 3 termos por query
            timeframe='today 12-m',
            geo='BR',
            gprop='web'
        )
    ]
    
    print(f"ğŸ“ˆ Consultas Google Trends criadas: {len(google_trends_queries)}")
    print(f"   - Keywords: {google_trends_queries[0].keywords}")
    
    # 3.4 CriaÃ§Ã£o de consulta de notÃ­cias baseada em keywords
    news_query = NewsScrapingQuery(
        keywords=top_keywords[:4],
        sources=['g1.globo.com', 'agenciabrasil.ebc.com.br', 'valor.globo.com'],
        max_articles=15,
        timeframe_days=30
    )
    
    print(f"ğŸ“° Consulta de notÃ­cias criada:")
    print(f"   - Keywords: {news_query.keywords}")
    print(f"   - Fontes: {len(news_query.sources)} configuradas")
    
    # ========================================================================
    # ETAPA 4: DOSSIÃŠ ETL FINAL (PONTE COMPLETA)
    # ========================================================================
    print("\nğŸŒ‰ ETAPA 4: DOSSIÃŠ ETL FINAL - PONTE COMPLETA")
    print("-" * 50)
    
    # CriaÃ§Ã£o do objeto ETLParameters final
    etl_params = ETLParameters(
        request_id=f"test_metodologia_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        user_input={
            "niche": niche,
            "description": description
        },
        nlp_features=nlp_features,
        ibge_queries=ibge_queries,
        google_trends_queries=google_trends_queries,
        news_queries=news_query,
        cache_enabled=True,
        max_retries=3
    )
    
    print(f"ğŸ“¦ DossiÃª ETL criado com sucesso!")
    print(f"   - Request ID: {etl_params.request_id}")
    print(f"   - Timestamp: {etl_params.timestamp}")
    
    # ========================================================================
    # ETAPA 5: VALIDAÃ‡ÃƒO DA PONTE (VERIFICAÃ‡Ã•ES DE INTEGRIDADE)
    # ========================================================================
    print("\nâœ… ETAPA 5: VALIDAÃ‡ÃƒO DA PONTE")
    print("-" * 50)
    
    # ValidaÃ§Ã£o 1: Palavras-chave do NLP â†’ Google Trends
    nlp_keywords = [k.keyword for k in nlp_features.keywords[:5]]
    trends_keywords = etl_params.google_trends_queries[0].keywords
    keyword_overlap = set(nlp_keywords) & set(trends_keywords)
    
    print(f"ğŸ” Palavras-chave NLP â†’ Google Trends:")
    print(f"   - NLP: {nlp_keywords}")
    print(f"   - Trends: {trends_keywords}")
    print(f"   - SobreposiÃ§Ã£o: {list(keyword_overlap)} ({'âœ…' if keyword_overlap else 'âŒ'})")
    
    # ValidaÃ§Ã£o 2: Entidades do NLP â†’ Filtro geogrÃ¡fico IBGE
    nlp_locations = [e.text for e in nlp_features.entities if e.label in ['LOC', 'GPE']]
    ibge_location = etl_params.ibge_queries[0].location
    location_match = any(loc.lower() in ibge_location.lower() for loc in nlp_locations) if nlp_locations else True
    
    print(f"ğŸŒ Entidades NLP â†’ Filtro geogrÃ¡fico IBGE:")
    print(f"   - NLP: {nlp_locations}")
    print(f"   - IBGE: {ibge_location}")
    print(f"   - CorrespondÃªncia: {'âœ…' if location_match else 'âŒ'}")
    
    # ValidaÃ§Ã£o 3: Keywords do NLP â†’ News scraping
    news_keywords = etl_params.news_queries.keywords
    news_overlap = set(nlp_keywords) & set(news_keywords)
    
    print(f"ğŸ“° Palavras-chave NLP â†’ News scraping:")
    print(f"   - NLP: {nlp_keywords}")
    print(f"   - News: {news_keywords}")
    print(f"   - SobreposiÃ§Ã£o: {list(news_overlap)} ({'âœ…' if news_overlap else 'âŒ'})")
    
    # ValidaÃ§Ã£o 4: Embeddings semÃ¢nticos disponÃ­veis
    embeddings_available = len(nlp_features.embeddings) > 0
    print(f"ğŸ§  Embeddings semÃ¢nticos:")
    print(f"   - Modelos: {list(nlp_features.embeddings.keys())}")
    print(f"   - DisponÃ­vel para busca por similaridade: {'âœ…' if embeddings_available else 'âŒ'}")
    
    # ValidaÃ§Ã£o 5: TraduÃ§Ã£o SIDRA funcionou
    sidra_mapped = 'tabela' in sidra_params and sidra_params['tabela'] is not None
    print(f"ğŸ”„ TraduÃ§Ã£o para SIDRA:")
    print(f"   - Mapeamento realizado: {'âœ…' if sidra_mapped else 'âŒ'}")
    print(f"   - Tabela: {sidra_params.get('tabela', 'N/A')}")
    
    # ========================================================================
    # RESUMO FINAL
    # ========================================================================
    print("\nğŸ¯ RESUMO FINAL: PONTE DE TRADUÃ‡ÃƒO INTELIGENTE")
    print("=" * 70)
    
    validations = [
        ("Palavras-chave â†’ Google Trends", keyword_overlap),
        ("Entidades â†’ Filtro geogrÃ¡fico", location_match),
        ("Keywords â†’ News scraping", news_overlap),
        ("Embeddings semÃ¢nticos", embeddings_available),
        ("TraduÃ§Ã£o SIDRA", sidra_mapped)
    ]
    
    all_valid = all(bool(validation[1]) for validation in validations)
    
    for validation_name, is_valid in validations:
        status = "âœ… FUNCIONANDO" if is_valid else "âŒ PROBLEMA"
        print(f"   {validation_name}: {status}")
    
    print(f"\nğŸŒ‰ PONTE DE TRADUÃ‡ÃƒO INTELIGENTE: {'âœ… IMPLEMENTADA' if all_valid else 'âŒ PROBLEMAS DETECTADOS'}")
    
    if all_valid:
        print("\nğŸ‰ SUCESSO! Toda a metodologia da ponte estÃ¡ funcionando:")
        print("   â€¢ NLP extrai features qualitativas â†’ âœ…")
        print("   â€¢ Mapper traduz para parÃ¢metros SIDRA â†’ âœ…")
        print("   â€¢ Orquestrador cria consultas parametrizadas â†’ âœ…")
        print("   â€¢ Embeddings disponÃ­veis para busca semÃ¢ntica â†’ âœ…")
        print("   â€¢ Filtros geogrÃ¡ficos aplicados â†’ âœ…")
        print("   â€¢ Pipeline pronto para coleta de dados externos â†’ âœ…")
    else:
        print("\nâš ï¸ PROBLEMAS DETECTADOS na implementaÃ§Ã£o da ponte!")
    
    return all_valid, etl_params

if __name__ == "__main__":
    success, etl_params = test_metodologia_ponte()
    
    print(f"\nğŸ“‹ OBJETO JSON FINAL (DOSSIÃŠ ETL):")
    print("-" * 50)
    
    # Serializa o objeto para JSON para mostrar a estrutura final
    etl_dict = etl_params.model_dump()
    
    # Mostra apenas as partes principais para nÃ£o poluir a saÃ­da
    key_parts = {
        "request_id": etl_dict["request_id"],
        "nlp_keywords": [k["keyword"] for k in etl_dict["nlp_features"]["keywords"][:5]],
        "nlp_entities": [e["text"] for e in etl_dict["nlp_features"]["entities"]],
        "ibge_table": etl_dict["ibge_queries"][0]["table_code"],
        "ibge_location": etl_dict["ibge_queries"][0]["location"],
        "trends_keywords": etl_dict["google_trends_queries"][0]["keywords"],
        "news_keywords": etl_dict["news_queries"]["keywords"],
        "embeddings_models": list(etl_dict["nlp_features"]["embeddings"].keys())
    }
    
    print(json.dumps(key_parts, indent=2, ensure_ascii=False))
    
    exit(0 if success else 1)
