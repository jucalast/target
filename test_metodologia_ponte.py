#!/usr/bin/env python3
"""
Teste da Metodologia da "Ponte de Tradu√ß√£o Inteligente"

Este script verifica se todo o processo descrito na metodologia est√° sendo
implementado corretamente, validando cada etapa da "ponte" entre NLP e ETL.
"""
import sys
import os

# Adiciona o backend ao path
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from shared.schemas.nlp_features import NLPFeatures, KeywordFeature, EntityFeature, TopicFeature, EmbeddingFeature
from shared.schemas.etl_parameters import ETLParameters, IBGETableQuery, GoogleTrendsQuery, NewsScrapingQuery
from etl_pipeline.app.services.extractors.ibge.sidra_mapper import SIDRAMapper
from nlp_processor.app.services.nlp_service import extract_features
import json
from datetime import datetime

def test_metodologia_ponte():
    """
    Testa todo o processo da "Ponte de Tradu√ß√£o Inteligente" descrito na metodologia.
    """
    print("üî¨ TESTE DA METODOLOGIA: PONTE DE TRADU√á√ÉO INTELIGENTE")
    print("=" * 70)
    
    # ========================================================================
    # ETAPA 1: PROCESSAMENTO NLP - GERA√á√ÉO DO "DOSSI√ä DE BUSCA"
    # ========================================================================
    print("\nüìù ETAPA 1: GERA√á√ÉO DO DOSSI√ä DE BUSCA (NLP)")
    print("-" * 50)
    
    # Input do usu√°rio (dados reais)
    niche = "Tecnologia educacional para crian√ßas"
    description = """
    Plataforma online de ensino de programa√ß√£o e rob√≥tica educativa para crian√ßas 
    de 6 a 12 anos, focada em escolas p√∫blicas e particulares do Brasil, 
    especialmente nas regi√µes Sul e Sudeste, com √™nfase em fam√≠lias de classe 
    m√©dia interessadas em educa√ß√£o STEAM.
    """
    
    print(f"üéØ Nicho: {niche}")
    print(f"üìÑ Descri√ß√£o: {description.strip()[:100]}...")
    
    # Executa o processamento NLP REAL
    nlp_result = extract_features(niche, description)
    
    # Converte para NLPFeatures estruturado
    nlp_features = NLPFeatures(
        original_text=f"{niche}. {description}",
        normalized_text=nlp_result.get('normalized_text', ''),
        keywords=[
            KeywordFeature(
                keyword=kw['keyword'],
                score=kw['score'],
                method='tfidf'
            ) for kw in nlp_result.get('keywords', [])
        ],
        entities=[
            EntityFeature(
                text=ent['text'],
                label=ent['label'],
                start_char=ent.get('start', 0),
                end_char=ent.get('end', 0)
            ) for ent in nlp_result.get('entities', [])
        ],
        topics=[
            TopicFeature(
                topic_id=i,
                keywords=[
                    {"word": kw, "score": 0.5} if isinstance(kw, str) 
                    else kw for kw in topic.get('keywords', [])
                ],
                score=topic.get('score', 0.0)
            ) for i, topic in enumerate(nlp_result.get('topics', []))
        ],
        embeddings={
            model: EmbeddingFeature(
                model=model,
                vector=vector if isinstance(vector, list) else [],
                dim=len(vector) if isinstance(vector, list) else 0
            )
            for model, vector in nlp_result.get('embeddings', {}).items()
            if isinstance(vector, list)  # Filtra apenas os vetores reais
        },
        processing_time=1.0
    )
    
    print(f"‚úÖ Palavras-chave extra√≠das: {len(nlp_features.keywords)}")
    print(f"   Top 5: {[k.keyword for k in nlp_features.keywords[:5]]}")
    print(f"‚úÖ Entidades extra√≠das: {len(nlp_features.entities)}")
    print(f"   Entidades: {[f'{e.text} ({e.label})' for e in nlp_features.entities[:3]]}")
    print(f"‚úÖ T√≥picos extra√≠dos: {len(nlp_features.topics)}")
    print(f"‚úÖ Embeddings gerados: {list(nlp_features.embeddings.keys())}")
    
    # Identifica entidades geogr√°ficas especificamente
    geographic_entities = [e.text for e in nlp_features.entities if e.label in ['LOC', 'GPE']]
    print(f"üåç Entidades geogr√°ficas: {geographic_entities}")
    
    # ========================================================================
    # ETAPA 2: TRADU√á√ÉO PARA PAR√ÇMETROS SIDRA (MAPPER)
    # ========================================================================
    print("\nüîÑ ETAPA 2: TRADU√á√ÉO PARA PAR√ÇMETROS SIDRA")
    print("-" * 50)
    
    # Extrai termos para mapeamento
    keywords_for_mapping = [k.keyword for k in nlp_features.keywords[:10]]
    
    print(f"üìã Input para mapeamento: {keywords_for_mapping}")
    
    # Executa o mapeamento REAL
    mapper = SIDRAMapper()
    sidra_params = mapper.map_terms_to_sidra_parameters(keywords_for_mapping)
    
    print(f"üéØ Tabela SIDRA selecionada: {sidra_params.get('tabela', 'N/A')}")
    print(f"üìä Par√¢metros mapeados:")
    
    matched_terms = sidra_params.get('matched_terms', {})
    for param_code, param_info in matched_terms.items():
        print(f"   - Par√¢metro {param_code}: {param_info.get('name', 'N/A')}")
        print(f"     Termos: {param_info.get('matched_terms', [])}")
    
    # ========================================================================
    # ETAPA 3: CONSTRU√á√ÉO DE PAR√ÇMETROS ETL (ORQUESTRA√á√ÉO)
    # ========================================================================
    print("\nüèóÔ∏è ETAPA 3: CONSTRU√á√ÉO DE PAR√ÇMETROS ETL")
    print("-" * 50)
    
    # 3.1 Filtro geogr√°fico baseado em entidades
    location = "Brasil"
    if geographic_entities:
        if any(geo.lower() in ['sul', 'sudeste', 'nordeste', 'norte', 'centro-oeste'] 
               for geo in geographic_entities):
            location = geographic_entities[0]
        elif any(geo.lower() == 'brasil' for geo in geographic_entities):
            location = "Brasil"
    
    print(f"üåç Filtro geogr√°fico aplicado: {location}")
    
    # 3.2 Cria√ß√£o de consultas IBGE baseadas na tradu√ß√£o
    ibge_queries = [
        IBGETableQuery(
            table_code=sidra_params.get('tabela', '6401'),
            variables=sidra_params.get('variaveis', []),
            classifications=sidra_params.get('classificacoes', {}),
            location=location,
            period='2022'
        )
    ]
    
    print(f"üìä Consultas IBGE criadas: {len(ibge_queries)}")
    print(f"   - Tabela: {ibge_queries[0].table_code}")
    print(f"   - Localiza√ß√£o: {ibge_queries[0].location}")
    
    # 3.3 Cria√ß√£o de consultas Google Trends baseadas em keywords
    top_keywords = [k.keyword for k in nlp_features.keywords[:5]]
    google_trends_queries = [
        GoogleTrendsQuery(
            keywords=top_keywords[:3],  # M√°ximo 3 termos por query
            timeframe='today 12-m',
            geo='BR',
            gprop='web'
        )
    ]
    
    print(f"üìà Consultas Google Trends criadas: {len(google_trends_queries)}")
    print(f"   - Keywords: {google_trends_queries[0].keywords}")
    
    # 3.4 Cria√ß√£o de consulta de not√≠cias baseada em keywords
    news_query = NewsScrapingQuery(
        keywords=top_keywords[:4],
        sources=['g1.globo.com', 'agenciabrasil.ebc.com.br', 'valor.globo.com'],
        max_articles=15,
        timeframe_days=30
    )
    
    print(f"üì∞ Consulta de not√≠cias criada:")
    print(f"   - Keywords: {news_query.keywords}")
    print(f"   - Fontes: {len(news_query.sources)} configuradas")
    
    # ========================================================================
    # ETAPA 4: DOSSI√ä ETL FINAL (PONTE COMPLETA)
    # ========================================================================
    print("\nüåâ ETAPA 4: DOSSI√ä ETL FINAL - PONTE COMPLETA")
    print("-" * 50)
    
    # Cria√ß√£o do objeto ETLParameters final
    etl_params = ETLParameters(
        request_id=f"test_metodologia_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        user_input={
            "niche": niche,
            "description": description
        },
        nlp_features=nlp_features,
        ibge_queries=ibge_queries,
        google_trends_queries=google_trends_queries,
        news_queries=news_query,
        cache_enabled=True,
        max_retries=3
    )
    
    print(f"üì¶ Dossi√™ ETL criado com sucesso!")
    print(f"   - Request ID: {etl_params.request_id}")
    print(f"   - Timestamp: {etl_params.timestamp}")
    
    # ========================================================================
    # ETAPA 5: VALIDA√á√ÉO DA PONTE (VERIFICA√á√ïES DE INTEGRIDADE)
    # ========================================================================
    print("\n‚úÖ ETAPA 5: VALIDA√á√ÉO DA PONTE")
    print("-" * 50)
    
    # Valida√ß√£o 1: Palavras-chave do NLP ‚Üí Google Trends
    nlp_keywords = [k.keyword for k in nlp_features.keywords[:5]]
    trends_keywords = etl_params.google_trends_queries[0].keywords
    keyword_overlap = set(nlp_keywords) & set(trends_keywords)
    
    print(f"üîç Palavras-chave NLP ‚Üí Google Trends:")
    print(f"   - NLP: {nlp_keywords}")
    print(f"   - Trends: {trends_keywords}")
    print(f"   - Sobreposi√ß√£o: {list(keyword_overlap)} ({'‚úÖ' if keyword_overlap else '‚ùå'})")
    
    # Valida√ß√£o 2: Entidades do NLP ‚Üí Filtro geogr√°fico IBGE
    nlp_locations = [e.text for e in nlp_features.entities if e.label in ['LOC', 'GPE']]
    ibge_location = etl_params.ibge_queries[0].location
    location_match = any(loc.lower() in ibge_location.lower() for loc in nlp_locations) if nlp_locations else True
    
    print(f"üåç Entidades NLP ‚Üí Filtro geogr√°fico IBGE:")
    print(f"   - NLP: {nlp_locations}")
    print(f"   - IBGE: {ibge_location}")
    print(f"   - Correspond√™ncia: {'‚úÖ' if location_match else '‚ùå'}")
    
    # Valida√ß√£o 3: Keywords do NLP ‚Üí News scraping
    news_keywords = etl_params.news_queries.keywords
    news_overlap = set(nlp_keywords) & set(news_keywords)
    
    print(f"üì∞ Palavras-chave NLP ‚Üí News scraping:")
    print(f"   - NLP: {nlp_keywords}")
    print(f"   - News: {news_keywords}")
    print(f"   - Sobreposi√ß√£o: {list(news_overlap)} ({'‚úÖ' if news_overlap else '‚ùå'})")
    
    # Valida√ß√£o 4: Embeddings sem√¢nticos dispon√≠veis
    embeddings_available = len(nlp_features.embeddings) > 0
    print(f"üß† Embeddings sem√¢nticos:")
    print(f"   - Modelos: {list(nlp_features.embeddings.keys())}")
    print(f"   - Dispon√≠vel para busca por similaridade: {'‚úÖ' if embeddings_available else '‚ùå'}")
    
    # Valida√ß√£o 5: Tradu√ß√£o SIDRA funcionou
    sidra_mapped = 'tabela' in sidra_params and sidra_params['tabela'] is not None
    print(f"üîÑ Tradu√ß√£o para SIDRA:")
    print(f"   - Mapeamento realizado: {'‚úÖ' if sidra_mapped else '‚ùå'}")
    print(f"   - Tabela: {sidra_params.get('tabela', 'N/A')}")
    
    # ========================================================================
    # RESUMO FINAL
    # ========================================================================
    print("\nüéØ RESUMO FINAL: PONTE DE TRADU√á√ÉO INTELIGENTE")
    print("=" * 70)
    
    validations = [
        ("Palavras-chave ‚Üí Google Trends", keyword_overlap),
        ("Entidades ‚Üí Filtro geogr√°fico", location_match),
        ("Keywords ‚Üí News scraping", news_overlap),
        ("Embeddings sem√¢nticos", embeddings_available),
        ("Tradu√ß√£o SIDRA", sidra_mapped)
    ]
    
    all_valid = all(bool(validation[1]) for validation in validations)
    
    for validation_name, is_valid in validations:
        status = "‚úÖ FUNCIONANDO" if is_valid else "‚ùå PROBLEMA"
        print(f"   {validation_name}: {status}")
    
    print(f"\nüåâ PONTE DE TRADU√á√ÉO INTELIGENTE: {'‚úÖ IMPLEMENTADA' if all_valid else '‚ùå PROBLEMAS DETECTADOS'}")
    
    if all_valid:
        print("\nüéâ SUCESSO! Toda a metodologia da ponte est√° funcionando:")
        print("   ‚Ä¢ NLP extrai features qualitativas ‚Üí ‚úÖ")
        print("   ‚Ä¢ Mapper traduz para par√¢metros SIDRA ‚Üí ‚úÖ")
        print("   ‚Ä¢ Orquestrador cria consultas parametrizadas ‚Üí ‚úÖ")
        print("   ‚Ä¢ Embeddings dispon√≠veis para busca sem√¢ntica ‚Üí ‚úÖ")
        print("   ‚Ä¢ Filtros geogr√°ficos aplicados ‚Üí ‚úÖ")
        print("   ‚Ä¢ Pipeline pronto para coleta de dados externos ‚Üí ‚úÖ")
    else:
        print("\n‚ö†Ô∏è PROBLEMAS DETECTADOS na implementa√ß√£o da ponte!")
    
    return all_valid, etl_params

if __name__ == "__main__":
    success, etl_params = test_metodologia_ponte()
    
    print(f"\nüìã OBJETO JSON FINAL (DOSSI√ä ETL):")
    print("-" * 50)
    
    # Serializa o objeto para JSON para mostrar a estrutura final
    etl_dict = etl_params.model_dump()
    
    # Mostra apenas as partes principais para n√£o poluir a sa√≠da
    key_parts = {
        "request_id": etl_dict["request_id"],
        "nlp_keywords": [k["keyword"] for k in etl_dict["nlp_features"]["keywords"][:5]],
        "nlp_entities": [e["text"] for e in etl_dict["nlp_features"]["entities"]],
        "ibge_table": etl_dict["ibge_queries"][0]["table_code"],
        "ibge_location": etl_dict["ibge_queries"][0]["location"],
        "trends_keywords": etl_dict["google_trends_queries"][0]["keywords"],
        "news_keywords": etl_dict["news_queries"]["keywords"],
        "embeddings_models": list(etl_dict["nlp_features"]["embeddings"].keys())
    }
    
    print(json.dumps(key_parts, indent=2, ensure_ascii=False))
    
    exit(0 if success else 1)
